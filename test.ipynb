{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ag_news_csv/test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadeich/env3/lib/python3.6/site-packages/ipykernel_launcher.py:1: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CharCNN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from metric import print_f_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../alphabet.json') as f:\n",
    "    res = json.load(f)\n",
    "num_features = len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    num_features = num_features\n",
    "    dropout = 0.5\n",
    "    test_path = '../data/ag_news_csv/test.csv'\n",
    "    alphabet_path = '../alphabet.json'\n",
    "    batch_size = 20\n",
    "    num_workers = 4\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model = CharCNN(args)\n",
    "checkpoint = torch.load('models/CharCNN_best.pth.tar')\n",
    "char_model.load_state_dict(checkpoint['state_dict'])\n",
    "char_model = char_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_dataset, model, limit=None):\n",
    "    corrects, avg_loss, accumulated_loss, size, fooled = 0, 0, 0, 0, 0\n",
    "    predicates_all, target_all = [], []\n",
    "    for i_batch, (data) in enumerate(tqdm(test_dataset)):\n",
    "        inputs_init, inputs, target = data\n",
    "        target.sub_(1)\n",
    "        size+=1\n",
    "        inputs_init = Variable(torch.unsqueeze(inputs_init, 0).to(\"cuda:0\"))\n",
    "        inputs = Variable(torch.unsqueeze(inputs, 0).to(\"cuda:0\"))\n",
    "        target = Variable(torch.unsqueeze(target, 0).to(\"cuda:0\"))\n",
    "        logit_init = model(inputs_init)\n",
    "        logit = model(inputs)\n",
    "        fooled += (logit_init.argmax() != logit.argmax()).detach().cpu().numpy()\n",
    "    \n",
    "        predicates = torch.max(logit, 1)[1].view(target.size()).data\n",
    "        accumulated_loss += F.nll_loss(logit, target, size_average=False).data.item()\n",
    "        corrects += (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum().item()\n",
    "        predicates_all+=predicates.cpu().numpy().tolist()\n",
    "        target_all+=target.data.cpu().numpy().tolist()\n",
    "        if limit is not None and i_batch == limit:\n",
    "            break\n",
    "\n",
    "    avg_loss = accumulated_loss/size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    fooled = 100.0 * fooled/size\n",
    "    print('\\rEvaluation - loss: {:.6f}  acc: {:.3f}%({}/{}) fooling rate: {:.3f}'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size,\n",
    "                                                                       fooled))\n",
    "    print_f_score(predicates_all, target_all)\n",
    "    return accuracy, fooled, predicates_all, target_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_hotflip_greedy import AGNEWs_HotFlip_Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a0195e09c14d329766930d9cbc1a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation - loss: 0.621648  acc: 81.818%(9/11) fooling rate: 18.182\n",
      "Label: \u001b[31m  2\u001b[0m\tPrec: \u001b[32m 50.0\u001b[0m% (1/2)        Recall: \u001b[32m100.0\u001b[0m% (1/1)        F-Score: \u001b[32m 66.7\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  3\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (8/8)        Recall: \u001b[32m 80.0\u001b[0m% (8/10)       F-Score: \u001b[32m 88.9\u001b[0m%\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fadeich/texts/char-cnn-text-classification-pytorch/final_code/model.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.log_softmax(x)\n",
      "/home/fadeich/env3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "test_dataset = AGNEWs_HotFlip_Greedy(per_corrupt=1, model=char_model, theta=0.5, dpp=False,\n",
    "                                  label_data_path=args.test_path, alphabet_path=args.alphabet_path)\n",
    "accuracy, fooled, predicates_all, target_all = test_model(test_dataset, char_model, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: fears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\n",
      "\u001b[31mcorrupt: \u001b[0mfears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' afte\u001b[31m=\u001b[0m talks with stricken parent firm federal mogul."
     ]
    }
   ],
   "source": [
    "test_dataset.print_string(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_hotflip_beam import AGNEWs_HotFlip_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41836cf1ec24984bee87c3cc5b13609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.884745  acc: 63.636%(7/11) fooling rate: 36.364\n",
      "Label: \u001b[31m  2\u001b[0m\tPrec: \u001b[32m 25.0\u001b[0m% (1/4)        Recall: \u001b[32m100.0\u001b[0m% (1/1)        F-Score: \u001b[32m 40.0\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  3\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (6/6)        Recall: \u001b[32m 60.0\u001b[0m% (6/10)       F-Score: \u001b[32m 75.0\u001b[0m%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_dataset_beam = AGNEWs_HotFlip_Beam(per_corrupt=1, model=char_model, theta=0.5, k=1, dpp=False,\n",
    "                                        label_data_path=args.test_path, alphabet_path=args.alphabet_path)\n",
    "accuracy, fooled, predicates_all, target_all = test_model(test_dataset_beam, char_model, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: fears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\n",
      "\u001b[31mcorrupt: \u001b[0mfears for t n pension after talks unions representing workers at turner   newall say they are 'disappointed' afte\u001b[31m=\u001b[0m talks with stricken parent firm federal mogul."
     ]
    }
   ],
   "source": [
    "test_dataset_beam.print_string(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_deepwordbug import WordBug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba3715016ee40e98f1dff866f825780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation - loss: 2.176774  acc: 36.364%(4/11) fooling rate: 63.636\n",
      "Label: \u001b[31m  2\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/6)        Recall: \u001b[32m  0.0\u001b[0m% (0/1)        F-Score:   N/A\u001b[0m\n",
      "Label: \u001b[31m  3\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (4/4)        Recall: \u001b[32m 40.0\u001b[0m% (4/10)       F-Score: \u001b[32m 57.1\u001b[0m%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_dataset_beam = WordBug(per_corrupt=4, model=char_model, scoring='beam',\n",
    "                                        label_data_path=args.test_path, alphabet_path=args.alphabet_path)\n",
    "accuracy, fooled, predicates_all, target_all = test_model(test_dataset_beam, char_model, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9cb909445a474480c9300c0a0a9e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation - loss: 1.300553  acc: 45.455%(5/11) fooling rate: 54.545\n",
      "Label: \u001b[31m  2\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/5)        Recall: \u001b[32m  0.0\u001b[0m% (0/1)        F-Score:   N/A\u001b[0m\n",
      "Label: \u001b[31m  3\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (5/5)        Recall: \u001b[32m 50.0\u001b[0m% (5/10)       F-Score: \u001b[32m 66.7\u001b[0m%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_dataset_beam = WordBug(per_corrupt=4, model=char_model, scoring='greedy',\n",
    "                                        label_data_path=args.test_path, alphabet_path=args.alphabet_path)\n",
    "accuracy, fooled, predicates_all, target_all = test_model(test_dataset_beam, char_model, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_hotflip_targeted import AGNEWs_HotFlip_Targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_beam = AGNEWs_HotFlip_Targeted(combine=False, per_corrupt=1, model=char_model, k=10,\n",
    "                                            label_data_path=args.test_path, alphabet_path=args.alphabet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c4119f06764f37acdc9ffec4c3f396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation - loss: 3.318901  acc: 14.286%(3/21) fooling rate: 28.571\n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (1/1)        Recall: \u001b[32m  8.3\u001b[0m% (1/12)       F-Score: \u001b[32m 15.4\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (1/1)        Recall: \u001b[32m 50.0\u001b[0m% (1/2)        F-Score: \u001b[32m 66.7\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  2\u001b[0m\tPrec: \u001b[32m 25.0\u001b[0m% (1/4)        Recall: \u001b[32m 16.7\u001b[0m% (1/6)        F-Score: \u001b[32m 20.0\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  3\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/15)       Recall: \u001b[32m  0.0\u001b[0m% (0/1)        F-Score:   N/A\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "accuracy, fooled, predicates_all, target_all = test_model(test_dataset_beam, char_model, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_beam = AGNEWs_HotFlip_Targeted(combine=True, per_corrupt=1, model=char_model, k=10,\n",
    "                                            label_data_path=args.test_path, alphabet_path=args.alphabet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86438273b2047c3b04994e6ecc0260b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluation - loss: 3.211607  acc: 14.286%(3/21) fooling rate: 33.333\n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (2/2)        Recall: \u001b[32m 16.7\u001b[0m% (2/12)       F-Score: \u001b[32m 28.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/0)        Recall: \u001b[32m  0.0\u001b[0m% (0/2)        F-Score:   N/A\u001b[0m\n",
      "Label: \u001b[31m  2\u001b[0m\tPrec: \u001b[32m 14.3\u001b[0m% (1/7)        Recall: \u001b[32m 16.7\u001b[0m% (1/6)        F-Score: \u001b[32m 15.4\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  3\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/12)       Recall: \u001b[32m  0.0\u001b[0m% (0/1)        F-Score:   N/A\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "accuracy, fooled, predicates_all, target_all = test_model(test_dataset_beam, char_model, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3 python",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
